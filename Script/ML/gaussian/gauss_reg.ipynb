{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b2398bd-b7b1-486b-96f7-5cd32ecc16cd",
   "metadata": {},
   "source": [
    "### There is more confusion on whether grid should be used in machine learning or are we using averages from dataset to setup a model\n",
    "### Untill that is clear, the ML works stops here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcc4199-646a-4945-abbe-a3a9274168e6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### The whole function of reading all simulations will be used only when all things are ready,\n",
    "####  for now it is of no use, so can be saved as raw."
   ]
  },
  {
   "cell_type": "raw",
   "id": "049de8e8-737d-43b2-9e5b-70bb040c4fa7",
   "metadata": {},
   "source": [
    "#bring all data\n",
    "#modify the function if you want to pass the parameter\n",
    "def read_all_simulation():\n",
    "    '''prepare cluster list and read to create ensemble(group of data)\n",
    "    use preprocess to select only certain dimension and a variable'''\n",
    "    # read all simulations as a list\n",
    "    cluster_list= sorted(glob.glob('/glade/campaign/cgd/tss/projects/PPE/PPEn11_LHC/transient/hist/PPEn11_transient_LHC[0][0-5][0-9][0-9].clm2.h0.2005-02-01-00000.nc'))\n",
    "    cluster_list = cluster_list[1:len(cluster_list)]\n",
    "\n",
    "    # only select latitude, longitude, time, and  using this in preprocess steps\n",
    "    def preprocess(ds):\n",
    "        '''using this function in xr.open_mfdataset as preprocess\n",
    "        ensures that when only these four things are selected \n",
    "        before the data is combined'''\n",
    "        return ds[['lat', 'lon', 'time', 'LEAFCN', 'TSA']]\n",
    "    \n",
    "    #read the list and load it for the notebook\n",
    "    data = xr.open_mfdataset( cluster_list, \n",
    "                                   combine='nested',\n",
    "                                   preprocess = lambda ds: preprocess(ds),\n",
    "                                   parallel=False, \n",
    "                                   concat_dim=\"ens\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a17bff2-b7fb-489d-9e66-5780f7e53702",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------steps involves\n",
    "#work with one piece of data only\n",
    "#split up the chunk\n",
    "#setup univariate gaussian \n",
    "#fit the gaussian cost function between leafcn and\n",
    "#plot the raw lines without any hyperparamter change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3271f35-0177-499e-8c0e-3d62e2207035",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read all the data and filter for only some\n",
    "from ml_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa7b70f-50cd-4b82-921c-21b3c1b4290b",
   "metadata": {},
   "source": [
    "#### The function is tested with single simulation of 1000 dataset only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "33a903e5-e98d-4f7a-abe9-4ab9ed0760a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a little different function than \n",
    "#what we have added in utils before, this \n",
    "#does not read require var to pass into and \n",
    "#and we should also not require that\\\n",
    "\n",
    "file='/glade/campaign/cgd/tss/projects/PPE/PPEn11_LHC/transient/hist/PPEn11_transient_LHC0001.clm2.h0.2005-02-01-00000.nc'\n",
    "latin_hypercube = xr.open_dataset(file)\n",
    "\n",
    "#bring in single data \n",
    "#subset only the TSA, and LeafCN from the dataset\n",
    "\n",
    "subset_latin_hypercube = latin_hypercube[[\"LEAFN\", \"LEAFCN\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "918bd38a-dc1e-4154-ba29-4eb03a2ad5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an array for both TSA and Leafcn\n",
    "tsa = subset_latin_hypercube[\"LEAFN\"].values\n",
    "leafcn = subset_latin_hypercube[\"LEAFCN\"].values\n",
    "\n",
    "leafn = subset_latin_hypercube[\"LEAFN\"].values\n",
    "leafcn = subset_latin_hypercube[\"LEAFCN\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "16b943c8-4034-470b-9e7e-8ffbe29ebf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten the list to single column, i tried without flattening that does not make sense\n",
    "leafn = np.array(leafn).flatten()\n",
    "leafcn = np.array(leafcn).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "22bcdc6a-db2a-4e75-8213-1e82766a1a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#there should be no missing values becuase machine does not learn anything from\n",
    "#that and it also does not how to interpret the data\n",
    "np.unique(leafn)\n",
    "np.isnan(leafcn).sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383b0a1b-7e8c-4edf-8193-0768a3c3f1e7",
   "metadata": {},
   "source": [
    "#### The K nearest method imputer makes more sense to me because i can use four nearby grid cells values\n",
    "#### to impute the nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "330b6f8b-7d26-4028-9bc7-6f5cd97a18a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Create a KNN imputer with k=4 (number of neighbors), not required for TSA as it no na values\n",
    "knn_imputer = KNNImputer(n_neighbors=4)\n",
    "leafcn= knn_imputer.fit_transform(np.array(leafcn).reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef486e1-8b1e-4847-b428-5faebcc14a9f",
   "metadata": {},
   "source": [
    "#### Data is cleaned, and it can be now fit using machine learning. But, this data should be shuffled\n",
    "#### otherwise there is specific grid and time and model won't perform better. So, do random arrnagement of the \n",
    "#### values keeping the relationships of input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "38fe4139-2c08-4a40-a75a-61e6fd2073bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming tsa and leafcn have the same length\n",
    "num_samples = len(leafn)\n",
    "indices = np.arange(num_samples)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Shuffle tsa and leafcn using the shuffled indices\n",
    "shuffled_leafn = leafn[indices]\n",
    "shuffled_leafcn = leafcn[indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecccb582-2096-4eb7-8334-9e4bb9c88cd5",
   "metadata": {},
   "source": [
    "#### Data preprocessing and selecting only 1000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "96224859-0387-4721-8d62-37dfb5821b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the dataset into xtrain and ytrain \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(shuffled_leafn, \n",
    "                                                    shuffled_leafcn, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "\n",
    "#---------use standard scaler to transform the dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Assuming tsa and leafcn are one-dimensional lists\n",
    "\n",
    "# Create StandardScaler objects\n",
    "train_x_scale= StandardScaler()\n",
    "\n",
    "# Fit the scalers to the data and transform the data\n",
    "X_train= train_x_scale.fit_transform(np.array(X_train).reshape(-1, 1))\n",
    "X_test = train_x_scale.fit_transform(np.array(X_test).reshape(-1, 1))\n",
    "\n",
    "#for working with ml subsetting only 1000 rows\n",
    "X_train = X_train[:1000]\n",
    "y_train = y_train[:1000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e2f186-c13a-4c17-9021-7a760fff06b6",
   "metadata": {},
   "source": [
    "#### This is based model with no hyperparameter changing, no cross validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "409337f4-8001-468f-96b0-9aa2eaf58f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/u/home/sbhattarai/.conda/envs/meds-py/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:434: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/glade/u/home/sbhattarai/.conda/envs/meds-py/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:424: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified lower bound 0.01. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Score: -915203.191497723\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#two packages that can be used to instantiate the gaussian model\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, RBF\n",
    "\n",
    "#instantiate the model and tune nothing in the beginning\n",
    "kernel = ConstantKernel(constant_value = 3, constant_value_bounds=(1e-2, 1e2)) \\\n",
    "              * RBF(length_scale=1, length_scale_bounds=(1e-2, 1e2))\n",
    "gp_model = GaussianProcessRegressor(kernel=kernel, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "gp_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "score = gp_model.score(X_test, y_test)\n",
    "print(\"Model Score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f06373-a3f9-44ba-8393-85921ac5b8c6",
   "metadata": {},
   "source": [
    "#### This is a model with 4 folds cross validation, neaning training 4 times from subsets\n",
    "#### for hyperparameter tuning, need to consult with Daniel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "811eac7e-fd3d-4856-bf04-779b75d590f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/u/home/sbhattarai/.conda/envs/meds-py/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:424: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/glade/u/home/sbhattarai/.conda/envs/meds-py/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:424: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/glade/u/home/sbhattarai/.conda/envs/meds-py/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:424: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/glade/u/home/sbhattarai/.conda/envs/meds-py/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:424: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-21.14571822 -29.31369955 -24.08539209 -30.16875394 -91.30085176]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/u/home/sbhattarai/.conda/envs/meds-py/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:424: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#use kfold validation to train the model\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import time\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 5\n",
    "\n",
    "#starttime for model run\n",
    "start_time = time.time()\n",
    "\n",
    "# Define the Gaussian Process model\n",
    "kernel = RBF()\n",
    "# kernel = ConstantKernel(constant_value = 3, constant_value_bounds=(1e-5, 1e5)) \\\n",
    "#               * RBF(length_scale=1, length_scale_bounds=(1e-5, 1e5))\n",
    "gp_model_cv = GaussianProcessRegressor(kernel=kernel, random_state=42, n_restarts_optimizer= 4)\n",
    "\n",
    "# Perform k-fold cross-validation and calculate MAE for each fold\n",
    "mae_scores = cross_val_score(gp_model_cv, X_train, y_train, cv = n_splits, scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(mae_scores)\n",
    "#endtime for model run\n",
    "end_time = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "d82efde6-4c67-4f27-adc4-8bec5d3c3616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I don't understand why model only predicting zeros, tried all possible approaches\n",
    "gp_model_cv.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-meds-py]",
   "language": "python",
   "name": "conda-env-.conda-meds-py-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
