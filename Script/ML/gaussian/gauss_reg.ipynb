{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b2398bd-b7b1-486b-96f7-5cd32ecc16cd",
   "metadata": {},
   "source": [
    "### There is more confusion on whether grid should be used in machine learning or are we using averages from dataset to setup a model\n",
    "### Untill that is clear, the ML works stops here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a7f7349-2a08-4687-87d4-4e98ea607131",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read all the data and filter for only some\n",
    "from ml_utils import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21f8e3f2-ed6e-442f-9a43-522312ef637b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/u/home/sbhattarai/.conda/envs/meds-py/lib/python3.11/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 37123 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#get the required clusters \n",
    "client = get_cluster(\"UCSB0021\", cores = 40)\n",
    "client.cluster;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a7603695-8476-4b6f-8fa7-f0c2fc38ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modify the function if you want to pass the parameter\n",
    "def read_all_simulation():\n",
    "    '''prepare cluster list and read to create ensemble(group of data)\n",
    "    use preprocess to select only certain dimension and a variable'''\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "\n",
    "    # read all simulations as a list\n",
    "    cluster_list = sorted(glob.glob('/glade/campaign/cgd/tss/projects/PPE/PPEn11_LHC/transient/hist/PPEn11_transient_LHC[0][0-5][0-9][0-9].clm2.h0.2005-02-01-00000.nc'))\n",
    "    cluster_list = cluster_list[1:len(cluster_list)]\n",
    "\n",
    "    # only select latitude, longitude, time, and using this in preprocess steps\n",
    "    def preprocess(ds):\n",
    "        '''using this function in xr.open_mfdataset as preprocess\n",
    "        ensures that when only these four things are selected\n",
    "        before the data is combined'''\n",
    "        ds = fix_time(ds)\n",
    "        ds = ds[['lat', 'lon', 'time', 'LEAFCN', 'LEAFN', 'TSA']]\n",
    "        return ds\n",
    "    \n",
    "    # read the list and load it for the notebook\n",
    "    ds = xr.open_mfdataset(cluster_list,\n",
    "                              combine='nested',\n",
    "                              preprocess=lambda ds: preprocess(ds),\n",
    "                              parallel=True,\n",
    "                              concat_dim=\"ens\")\n",
    "\n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Time taken to run read_all_simulation(): {:.2f} seconds\".format(elapsed_time))\n",
    "    return ds\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e060ad6f-2158-4a62-bde0-bcffe663e407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run read_all_simulation(): 12.02 seconds\n"
     ]
    }
   ],
   "source": [
    "ds = read_all_simulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639078a0-5abc-4df8-aa04-5dcebfc47949",
   "metadata": {},
   "source": [
    "**The weight_landarea_gridcells function available inside the utils is designed only for dataarray, \n",
    "I am changing to work with dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "315e8d53-59ed-4462-8841-109d584454ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the landarea data\n",
    "landarea = xr.open_dataset('/glade/campaign/cgd/tss/projects/PPE/helpers/sparsegrid_landarea.nc')['landarea']\n",
    "\n",
    "def weight_landarea_gridcells(ds, landarea):\n",
    "    for varname in ds.variables:\n",
    "        # Skip coordinates and auxiliary variables\n",
    "        if varname not in ds.coords:\n",
    "            var = ds[varname]\n",
    "            # Calculate the weighted mean of the variable over grid cells\n",
    "            weighted_mean = var.weighted(landarea).mean(dim='gridcell')\n",
    "            # Assign the weighted mean back to the dataset\n",
    "            ds[varname] = weighted_mean\n",
    "    return ds\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e36901-7108-405d-a25a-2c8587c43734",
   "metadata": {},
   "source": [
    "**The dataset is weighted by gridcells at this point** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bb25a9c0-68db-4404-a72c-5d6d8a3233fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = weight_landarea_gridcells(ds, landarea)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b63613a-1aad-4bff-9410-5f577b77b889",
   "metadata": {},
   "source": [
    "**Weight the dataset based on yearly average. Note: these functions are designed to workwith dataset and not with datarray and these functions are also different from what is available in utils. We have to change in the utils later**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c67378c4-b86a-4aea-945c-9b71642b32b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yearly_weighted_average(ds):\n",
    "    for varname in ds.variables:\n",
    "        # Skip coordinates and auxiliary variables\n",
    "        if varname not in ds.coords:\n",
    "            var = ds[varname]\n",
    "            # Get the array of number of days from the main dataset\n",
    "            days_in_month = ds['time.daysinmonth']\n",
    "\n",
    "            # # Multiply each month's data by corresponding days in month\n",
    "            weighted_month = (days_in_month * var).groupby(\"time.year\").sum(dim='time')\n",
    "\n",
    "            # # Total days in the year\n",
    "            total_days = days_in_month.groupby(\"time.year\").sum(dim='time')\n",
    "\n",
    "            # # Calculate weighted average for the year\n",
    "            ds[varname] = weighted_month / total_days\n",
    "\n",
    "    return ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1e4be0bb-4fff-4a1f-a98c-5d4474b7f9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_final  = yearly_weighted_average(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "36555e4e-176b-4818-a1e1-63bc8e14435c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate the total average\n",
    "ds_test = ds_final\n",
    "\n",
    "year = ds_test[\"time\"].dt.year\n",
    "decade = ((year - year[0]) / 5).astype(int)\n",
    "ds_test[\"decade\"] = decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9c26ce36-ebc4-48df-a65f-7a8e7a800b3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reduce over dimensions ['decade']. expected either '...' to reduce over all dimensions or one or more of FrozenMappingWarningOnValuesAccess(FrozenMappingWarningOnValuesAccess({'lat': 96, 'lon': 144, 'time': 60, 'year': 5})). Try passing .groupby(..., squeeze=False)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ds_test \u001b[38;5;241m=\u001b[39m ds_test\u001b[38;5;241m.\u001b[39mset_coords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecade\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m ds_test \u001b[38;5;241m=\u001b[39m \u001b[43mds_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecade\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m ds_test\n",
      "File \u001b[0;32m~/.conda/envs/meds-py/lib/python3.11/site-packages/xarray/core/_aggregations.py:2985\u001b[0m, in \u001b[0;36mDatasetGroupByAggregations.mean\u001b[0;34m(self, dim, skipna, keep_attrs, **kwargs)\u001b[0m\n\u001b[1;32m   2975\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flox_reduce(\n\u001b[1;32m   2976\u001b[0m         func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2977\u001b[0m         dim\u001b[38;5;241m=\u001b[39mdim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2982\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2983\u001b[0m     )\n\u001b[1;32m   2984\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce_without_squeeze_warn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2986\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduck_array_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2987\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2988\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2989\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2990\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2991\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2992\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/meds-py/lib/python3.11/site-packages/xarray/core/groupby.py:1862\u001b[0m, in \u001b[0;36mDatasetGroupByBase._reduce_without_squeeze_warn\u001b[0;34m(self, func, dim, axis, keep_attrs, keepdims, shortcut, **kwargs)\u001b[0m\n\u001b[1;32m   1860\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[1;32m   1861\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `squeeze` kwarg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1862\u001b[0m     \u001b[43mcheck_reduce_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_maybe_warn(reduce_dataset, warn_squeeze\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/meds-py/lib/python3.11/site-packages/xarray/core/groupby.py:71\u001b[0m, in \u001b[0;36mcheck_reduce_dims\u001b[0;34m(reduce_dims, dimensions)\u001b[0m\n\u001b[1;32m     69\u001b[0m     reduce_dims \u001b[38;5;241m=\u001b[39m [reduce_dims]\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(dim \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dimensions \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m reduce_dims):\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot reduce over dimensions \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreduce_dims\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m. expected either \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto reduce over all dimensions or one or more of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdimensions\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Try passing .groupby(..., squeeze=False)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reduce over dimensions ['decade']. expected either '...' to reduce over all dimensions or one or more of FrozenMappingWarningOnValuesAccess(FrozenMappingWarningOnValuesAccess({'lat': 96, 'lon': 144, 'time': 60, 'year': 5})). Try passing .groupby(..., squeeze=False)"
     ]
    }
   ],
   "source": [
    "ds_test = ds_test.set_coords(\"decade\")\n",
    "ds_test = ds_test.groupby(\"ens\").mean(dim = \"decade\")\n",
    "ds_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb06e62-9289-4477-858e-c88a562e5574",
   "metadata": {},
   "source": [
    "**Make sure not to run single chunk for above functions, either run all of them or none**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcc4199-646a-4945-abbe-a3a9274168e6",
   "metadata": {},
   "source": [
    "##### The whole function of reading all simulations will be used only when all things are ready, for now it is of no use, so can be saved as raw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a17bff2-b7fb-489d-9e66-5780f7e53702",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------steps involves\n",
    "#work with one piece of data only\n",
    "#split up the chunk\n",
    "#setup univariate gaussian \n",
    "#fit the gaussian cost function between leafcn and\n",
    "#plot the raw lines without any hyperparamter change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa7b70f-50cd-4b82-921c-21b3c1b4290b",
   "metadata": {},
   "source": [
    "#### The function is tested with single simulation of 1000 dataset only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918bd38a-dc1e-4154-ba29-4eb03a2ad5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an array for both leafn and Leafcn\n",
    "leafn =  ds_final[\"LEAFN\"].values\n",
    "leafcn = ds_final[\"LEAFCN\"].values\n",
    "\n",
    "lat = ds_final['lat'].values\n",
    "lon = ds_final ['lon'].values\n",
    "time = ds_final ['time'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a0f6a2-3f37-4d35-9a16-91100881596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b943c8-4034-470b-9e7e-8ffbe29ebf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten the list to single column, i tried without flattening that does not make sense\n",
    "leafn = np.array(leafn).flatten()\n",
    "leafcn = np.array(leafcn).flatten()\n",
    "\n",
    "leafn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bcdc6a-db2a-4e75-8213-1e82766a1a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#there should be no missing values becuase machine does not learn anything from\n",
    "#that and it also does not how to interpret the data\n",
    "np.unique(leafn)\n",
    "np.isnan(leafcn).sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383b0a1b-7e8c-4edf-8193-0768a3c3f1e7",
   "metadata": {},
   "source": [
    "#### The K nearest method imputer makes more sense to me because i can use four nearby grid cells values\n",
    "#### to impute the nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330b6f8b-7d26-4028-9bc7-6f5cd97a18a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Create a KNN imputer with k=4 (number of neighbors), not required for leafn as it no na values\n",
    "knn_imputer = KNNImputer(n_neighbors=4)\n",
    "leafcn= knn_imputer.fit_transform(np.array(leafcn).reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef486e1-8b1e-4847-b428-5faebcc14a9f",
   "metadata": {},
   "source": [
    "#### Data is cleaned, and it can be now fit using machine learning. But, this data should be shuffled\n",
    "#### otherwise there is specific grid and time and model won't perform better. So, do random arrnagement of the \n",
    "#### values keeping the relationships of input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fe4139-2c08-4a40-a75a-61e6fd2073bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming leafn and leafcn have the same length\n",
    "num_samples = len(leafn)\n",
    "indices = np.arange(num_samples)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Shuffle leafn and leafcn using the shuffled indices\n",
    "shuffled_leafn = leafn[indices]\n",
    "shuffled_leafcn = leafcn[indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecccb582-2096-4eb7-8334-9e4bb9c88cd5",
   "metadata": {},
   "source": [
    "#### Data preprocessing and selecting only 1000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96224859-0387-4721-8d62-37dfb5821b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into xtrain and ytrain \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(shuffled_leafn, \n",
    "                                                    shuffled_leafcn, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# #---------use standard scaler to transform the dataset\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Create StandardScaler objects\n",
    "# train_x_scale= StandardScaler()\n",
    "\n",
    "# # Fit the scalers to the data and transform the data\n",
    "X_train= np.array(X_train).reshape(-1, 1)\n",
    "X_test =  np.array(X_test).reshape(-1, 1)\n",
    "\n",
    "#for working with ml subsetting only 1000 rows\n",
    "y_train = X_train[:1000]\n",
    "y_train = y_train[:1000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e2f186-c13a-4c17-9021-7a760fff06b6",
   "metadata": {},
   "source": [
    "#### This is based model with no hyperparameter changing, no cross validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409337f4-8001-468f-96b0-9aa2eaf58f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#two packages that can be used to instantiate the gaussian model\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, RBF\n",
    "\n",
    "#instantiate the model and tune nothing in the beginning\n",
    "kernel = ConstantKernel(constant_value = 3, constant_value_bounds=(1e-2, 1e2)) \\\n",
    "              * RBF(length_scale=1, length_scale_bounds=(1e-8, 1e8))\n",
    "gp_model = GaussianProcessRegressor(kernel=kernel, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "gp_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "score = gp_model.score(X_test, y_test)\n",
    "print(\"Model Score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f06373-a3f9-44ba-8393-85921ac5b8c6",
   "metadata": {},
   "source": [
    "#### This is a model with 4 folds cross validation, neaning training 4 times from subsets\n",
    "#### for hyperparameter tuning, need to consult with Daniel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811eac7e-fd3d-4856-bf04-779b75d590f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use kfold validation to train the model\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import time\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 5\n",
    "\n",
    "#starttime for model run\n",
    "start_time = time.time()\n",
    "\n",
    "# Define the Gaussian Process model\n",
    "kernel = RBF()\n",
    "# kernel = ConstantKernel(constant_value = 3, constant_value_bounds=(1e-5, 1e5)) \\\n",
    "#               * RBF(length_scale=1, length_scale_bounds=(1e-5, 1e5))\n",
    "gp_model_cv = GaussianProcessRegressor(kernel=kernel, random_state=42, n_restarts_optimizer= 4)\n",
    "\n",
    "# Perform k-fold cross-validation and calculate MAE for each fold\n",
    "mae_scores = cross_val_score(gp_model_cv, X_train, y_train, cv = n_splits, scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(mae_scores)\n",
    "#endtime for model run\n",
    "end_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82efde6-4c67-4f27-adc4-8bec5d3c3616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't understand why model only predicting zeros, tried all possible approaches\n",
    "gp_model_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f602c27-7a23-4c63-b2aa-d3c800be075f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-meds-py]",
   "language": "python",
   "name": "conda-env-.conda-meds-py-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
