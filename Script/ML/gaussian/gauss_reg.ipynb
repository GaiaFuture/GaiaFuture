{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b2398bd-b7b1-486b-96f7-5cd32ecc16cd",
   "metadata": {},
   "source": [
    "### There is more confusion on whether grid should be used in machine learning or are we using averages from dataset to setup a model\n",
    "### Untill that is clear, the ML works stops here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a7f7349-2a08-4687-87d4-4e98ea607131",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read all the data and filter for only some\n",
    "from utils import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21f8e3f2-ed6e-442f-9a43-522312ef637b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/u/home/sbhattarai/.conda/envs/meds-py/lib/python3.11/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 38323 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b1913b508741259cad370b7376db56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\">\n",
       "    </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">PBSCluster</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">a2a18890</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"https://jupyterhub.hpc.ucar.edu/stable/user/sbhattarai/proxy/38323/status\" target=\"_blank\">https://jupyterhub.hpc.ucar.edu/stable/user/sbhattarai/proxy/38323/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Workers:</strong> 0\n",
       "                </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong> 0\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong> 0 B\n",
       "                </td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "\n",
       "        <details>\n",
       "            <summary style=\"margin-bottom: 20px;\">\n",
       "                <h3 style=\"display: inline;\">Scheduler Info</h3>\n",
       "            </summary>\n",
       "\n",
       "            <div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-f3a0e6a4-3c89-4df0-aa46-0313f13b6a74</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> tcp://128.117.208.103:35201\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"https://jupyterhub.hpc.ucar.edu/stable/user/sbhattarai/proxy/38323/status\" target=\"_blank\">https://jupyterhub.hpc.ucar.edu/stable/user/sbhattarai/proxy/38323/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> Just now\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 0 B\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "\n",
       "        </details>\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "PBSCluster(a2a18890, 'tcp://128.117.208.103:35201', workers=0, threads=0, memory=0 B)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get the required clusters \n",
    "client = get_cluster(\"UCSB0021\", cores = 40)\n",
    "client.cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7603695-8476-4b6f-8fa7-f0c2fc38ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_simulation(var):\n",
    "    '''prepare cluster list and read to create ensemble(group of data)\n",
    "    use preprocess to select only certain dimension and a variable'''\n",
    "\n",
    "    # read all simulations as a list\n",
    "    cluster_list = sorted(glob.glob('/glade/campaign/cgd/tss/projects/PPE/PPEn11_LHC/transient/hist/PPEn11_transient_LHC[0][0-5][0-9][0-9].clm2.h0.2005-02-01-00000.nc'))\n",
    "    cluster_list = cluster_list[1:len(cluster_list)]\n",
    "\n",
    "    # only select latitude, longitude, time, and the user-specified variable\n",
    "    def preprocess(ds):\n",
    "        '''using this function in xr.open_mfdataset as preprocess\n",
    "        ensures that when only these four things are selected\n",
    "        before the data is combined'''\n",
    "        ds = fix_time(ds)\n",
    "        ds = ds[['lat', 'lon', 'time', var]]\n",
    "        return ds\n",
    "    \n",
    "    # read the list and load it for the notebook\n",
    "    ds = xr.open_mfdataset(cluster_list,\n",
    "                              combine='nested',\n",
    "                              preprocess=lambda ds: preprocess(ds),\n",
    "                              parallel=True,\n",
    "                              concat_dim=\"ens\")\n",
    "\n",
    "    return ds\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9d0a285-d2d3-4d46-9018-3ff97e790dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.4151036e-01 4.2529511e-01 1.4905481e-01 ... 2.7109760e-05 4.9810752e-02\n",
      " 1.5072618e-01]\n"
     ]
    }
   ],
   "source": [
    "#new added\n",
    "\n",
    "def read_all_simulation(var):\n",
    "    '''prepare cluster list and read to create ensemble(group of data)\n",
    "    use preprocess to select only certain dimension and a variable'''\n",
    "\n",
    "    # read all simulations as a list\n",
    "    cluster_list = sorted(glob.glob('/glade/campaign/cgd/tss/projects/PPE/PPEn11_LHC/transient/hist/PPEn11_transient_LHC[0][0-5][0-9][0-9].clm2.h0.2005-02-01-00000.nc'))\n",
    "    cluster_list = cluster_list[1:len(cluster_list)]\n",
    "\n",
    "    # only select latitude, longitude, time, and the user-specified variable\n",
    "    def preprocess(ds):\n",
    "        '''using this function in xr.open_mfdataset as preprocess\n",
    "        ensures that when only these four things are selected\n",
    "        before the data is combined'''\n",
    "        ds = fix_time(ds)\n",
    "        ds = ds[['lat', 'lon', 'time', var]]\n",
    "        return ds\n",
    "    \n",
    "    # read the list and load it for the notebook\n",
    "    ds = xr.open_mfdataset(cluster_list,\n",
    "                              combine='nested',\n",
    "                              preprocess=lambda ds: preprocess(ds),\n",
    "                              parallel=True,\n",
    "                              concat_dim=\"ens\")\n",
    "\n",
    "    return ds[var].values.flatten()\n",
    "\n",
    "# Example usage\n",
    "data_array = read_all_simulation(\"H2OSOI\")\n",
    "\n",
    "print(data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8171d849-3d34-436d-965c-02edd43af5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "639078a0-5abc-4df8-aa04-5dcebfc47949",
   "metadata": {},
   "source": [
    "**The weight_landarea_gridcells function available inside the utils is designed only for dataarray, \n",
    "I am changing to work with dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "315e8d53-59ed-4462-8841-109d584454ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the landarea data\n",
    "landarea = xr.open_dataset('/glade/campaign/cgd/tss/projects/PPE/helpers/sparsegrid_landarea.nc')['landarea']\n",
    "\n",
    "def weight_landarea_gridcells(ds, landarea):\n",
    "    for varname in ds.variables:\n",
    "        # Skip coordinates and auxiliary variables\n",
    "        if varname not in ds.coords:\n",
    "            var = ds[varname]\n",
    "            # Calculate the weighted mean of the variable over grid cells\n",
    "            weighted_mean = var.weighted(landarea).mean(dim='gridcell')\n",
    "            # Assign the weighted mean back to the dataset\n",
    "            ds[varname] = weighted_mean\n",
    "    return ds\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e36901-7108-405d-a25a-2c8587c43734",
   "metadata": {},
   "source": [
    "**The dataset is weighted by gridcells at this point** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb25a9c0-68db-4404-a72c-5d6d8a3233fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = weight_landarea_gridcells(ds, landarea)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b63613a-1aad-4bff-9410-5f577b77b889",
   "metadata": {},
   "source": [
    "**Weight the dataset based on yearly average. Note: these functions are designed to workwith dataset and not with datarray and these functions are also different from what is available in utils. We have to change in the utils later**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c67378c4-b86a-4aea-945c-9b71642b32b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yearly_weighted_average(ds):\n",
    "    for varname in ds.variables:\n",
    "        # Skip coordinates and auxiliary variables\n",
    "        if varname not in ds.coords:\n",
    "            var = ds[varname]\n",
    "            # Get the array of number of days from the main dataset\n",
    "            days_in_month = ds['time.daysinmonth']\n",
    "\n",
    "            # # Multiply each month's data by corresponding days in month\n",
    "            weighted_month = (days_in_month * var).groupby(\"time.year\").sum(dim='time')\n",
    "\n",
    "            # # Total days in the year\n",
    "            total_days = days_in_month.groupby(\"time.year\").sum(dim='time')\n",
    "\n",
    "            #  Calculate weighted average for the year\n",
    "            ds[varname] = weighted_month / total_days\n",
    "\n",
    "            #appy total average here\n",
    "            ds[varname] = ds[varname].mean(dim = \"year\")\n",
    "\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e4be0bb-4fff-4a1f-a98c-5d4474b7f9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (lat: 96, lon: 144, time: 60, ens: 500, levsoi: 20, year: 5)\n",
      "Coordinates:\n",
      "  * lat      (lat) float32 -90.0 -88.11 -86.21 -84.32 ... 84.32 86.21 88.11 90.0\n",
      "  * lon      (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5\n",
      "  * time     (time) object 2005-01-01 00:00:00 ... 2009-12-01 00:00:00\n",
      "  * levsoi   (levsoi) float32 0.01 0.04 0.09 0.16 0.26 ... 5.06 5.95 6.94 8.03\n",
      "  * year     (year) int64 2005 2006 2007 2008 2009\n",
      "Dimensions without coordinates: ens\n",
      "Data variables:\n",
      "    H2OSOI   (ens, levsoi) float64 dask.array<chunksize=(1, 20), meta=np.ndarray>\n",
      "Attributes: (12/40)\n",
      "    title:                                     CLM History file information\n",
      "    comment:                                   NOTE: None of the variables ar...\n",
      "    Conventions:                               CF-1.0\n",
      "    history:                                   created on 08/10/23 00:09:45\n",
      "    source:                                    Community Terrestrial Systems ...\n",
      "    hostname:                                  cheyenne\n",
      "    ...                                        ...\n",
      "    ctype_urban_pervious_road:                 75\n",
      "    cft_c3_crop:                               1\n",
      "    cft_c3_irrigated:                          2\n",
      "    time_period_freq:                          month_1\n",
      "    Time_constant_3Dvars_filename:             ./PPEn11_transient_LHC0001.clm...\n",
      "    Time_constant_3Dvars:                      ZSOI:DZSOI:WATSAT:SUCSAT:BSW:H...\n"
     ]
    }
   ],
   "source": [
    "ds_final  = yearly_weighted_average(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9256a9d2-92a0-401d-a9b4-990b93e5f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read all parameteres for 500 simulations\n",
    "parameters = pd.read_csv('/glade/campaign/asp/djk2120/PPEn11/csvs/lhc220926.txt',index_col=0)\n",
    "params = xr.Dataset(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91eff82-cdcd-469f-9de0-815a06283944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookup values that is later required in figure labelling \n",
    "data = {\n",
    "    'Index': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n",
    "              21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],\n",
    "    'Parameter': ['FUN_fracfixers', 'KCN', 'a_fix', 'crit_dayl', 'd_max', 'fff',\n",
    "                  'froot_leaf', 'fstor2tran', 'grperc', 'jmaxb0', 'jmaxb1', 'kcha', 'kmax',\n",
    "                  'krmax', 'leaf_long', 'leafcn', 'lmr_intercept_atkin', 'lmrha', 'lmrhd',\n",
    "                  'medlynintercept', 'medlynslope', 'nstem', 'psi50', 'q10_mr', 'slatop',\n",
    "                  'soilpsi_off', 'stem_leaf', 'sucsat_sf', 'theta_cj', 'tpu25ratio',\n",
    "                  'tpuse_sf', 'wc2wjb0']\n",
    "}\n",
    "names_of_parameters = pd.DataFrame(data)\n",
    "\n",
    "# Set 'Index' column as the index and convert to dictionary\n",
    "names_of_parameters_dict = names_of_parameters.set_index('Index')['Parameter'].to_dict()\n",
    "print(names_of_parameters_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb06e62-9289-4477-858e-c88a562e5574",
   "metadata": {},
   "source": [
    "**Make sure not to run single chunk for above functions, either run all of them or none**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcc4199-646a-4945-abbe-a3a9274168e6",
   "metadata": {},
   "source": [
    "##### The whole function of reading all simulations will be used only when all things are ready, for now it is of no use, so can be saved as raw."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa7b70f-50cd-4b82-921c-21b3c1b4290b",
   "metadata": {},
   "source": [
    "#### The function is tested with single simulation of 1000 dataset only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c17d6a-e900-45e5-bc01-2e41cac2aa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an array for both leafn and Leafcn\n",
    "leafn =  np.array(ds_final[\"LEAFCN\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918bd38a-dc1e-4154-ba29-4eb03a2ad5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "###creating dataset for all 32 params\n",
    "def array_of_params(params):\n",
    "    # Get variable names, selecting from 1 because a column called 'member' pops out from nowhere there that creates problem in function\n",
    "    variable_names = list(params.variables)[1:]\n",
    "    \n",
    "    # Initialize an empty list to store arrays, this is important becuase final dataset should contain 32 columns\n",
    "    arrays = []\n",
    "    \n",
    "    # Iterate over each variable\n",
    "    for var_name in variable_names:\n",
    "        # Extract values of the variable as a NumPy array\n",
    "        values = np.array(params[var_name])\n",
    "        # Ensure that the array has a single column\n",
    "        if len(values.shape) == 1:\n",
    "            values = values[:, np.newaxis]\n",
    "        arrays.append(values)\n",
    "    \n",
    "    # Concatenate all arrays along the second axis (column-wise)\n",
    "    concatenated_array = np.concatenate(arrays, axis=1)\n",
    "    \n",
    "    return concatenated_array\n",
    "\n",
    "#run the fucntion\n",
    "array_of_params(params).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d364ffc9-f862-44f9-b081-446b3af831da",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'array_of_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m params_array \u001b[38;5;241m=\u001b[39m \u001b[43marray_of_params\u001b[49m(params)\n\u001b[1;32m      2\u001b[0m params_array\n",
      "\u001b[0;31mNameError\u001b[0m: name 'array_of_params' is not defined"
     ]
    }
   ],
   "source": [
    "params_array = array_of_params(params)\n",
    "params_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383b0a1b-7e8c-4edf-8193-0768a3c3f1e7",
   "metadata": {},
   "source": [
    "#### The K nearest method imputer makes more sense to me because I can use four nearby grid cells values\n",
    "#### to impute the nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330b6f8b-7d26-4028-9bc7-6f5cd97a18a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.impute import KNNImputer\n",
    "\n",
    "# # Create a KNN imputer with k=4 (number of neighbors), not required for leafn as it no na values\n",
    "# knn_imputer = KNNImputer(n_neighbors=4)\n",
    "# leafcn= knn_imputer.fit_transform(np.array(leafcn).reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef486e1-8b1e-4847-b428-5faebcc14a9f",
   "metadata": {},
   "source": [
    "#### Data is cleaned, and it can be now fit using machine learning. But, this data should be shuffled\n",
    "#### otherwise there is specific grid and time and model won't perform better. So, do random arrnagement of the \n",
    "#### values keeping the relationships of input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fe4139-2c08-4a40-a75a-61e6fd2073bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming leafn and leafcn have the same length\n",
    "# num_samples = len(leafn)\n",
    "# indices = np.arange(num_samples)\n",
    "# np.random.shuffle(indices)\n",
    "\n",
    "# # Shuffle leafn and leafcn using the shuffled indices\n",
    "# shuffled_leafn = leafn[indices]\n",
    "# shuffled_leafcn = leafcn[indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecccb582-2096-4eb7-8334-9e4bb9c88cd5",
   "metadata": {},
   "source": [
    "#### Data preprocessing and selecting only 1000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96224859-0387-4721-8d62-37dfb5821b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into xtrain and ytrain \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(params_array, \n",
    "                                                    leafn, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e2f186-c13a-4c17-9021-7a760fff06b6",
   "metadata": {},
   "source": [
    "#### This is based model with no hyperparameter changing, no cross validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409337f4-8001-468f-96b0-9aa2eaf58f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #two packages that can be used to instantiate the gaussian model\n",
    "# from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "# from sklearn.gaussian_process.kernels import RBF\n",
    "# from sklearn.gaussian_process.kernels import ConstantKernel, RBF\n",
    "\n",
    "#instantiate the model and tune nothing in the beginning\n",
    "kernel = ConstantKernel(constant_value = 3, constant_value_bounds=(1e-2, 1e4)) \\\n",
    "              * RBF(length_scale=1, length_scale_bounds=(1e-4, 1e8))\n",
    "gp_model = GaussianProcessRegressor(kernel=kernel, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "gp_model.fit(X_train, y_train)\n",
    "\n",
    "# Verify training score\n",
    "train_score = gp_model.score(X_train, y_train)\n",
    "print(\"Training Score:\", train_score)\n",
    "\n",
    "predictions = gp_model.predict(X_test)\n",
    "\n",
    "#gp_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f06373-a3f9-44ba-8393-85921ac5b8c6",
   "metadata": {},
   "source": [
    "#### 4 fold cross validation in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811eac7e-fd3d-4856-bf04-779b75d590f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use kfold validation to train the model\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import time\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 5\n",
    "\n",
    "#starttime for model run\n",
    "start_time = time.time()\n",
    "\n",
    "# Define the Gaussian Process model\n",
    "#kernel = RBF(length_scale_bounds=\"fixed\")\n",
    "kernel = RBF()\n",
    "\n",
    "# kernel = ConstantKernel(constant_value = 3, constant_value_bounds=(1e-5, 1e5)) \\\n",
    "#               * RBF(length_scale=1, length_scale_bounds=(1e-5, 1e5))\n",
    "gp_model_cv = GaussianProcessRegressor(kernel=kernel, random_state=42, n_restarts_optimizer= 4,\n",
    "                                       alpha= 0.5, normalize_y = True)\n",
    "\n",
    "# Perform k-fold cross-validation and calculate MAE for each fold\n",
    "mae_scores = cross_val_score(gp_model_cv, X_train, y_train, cv = n_splits, scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(mae_scores)\n",
    "\n",
    "gp_model_cv.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gp_model_cv.predict(X_test)\n",
    "\n",
    "# Calculate Mean Absolute Error on the test set\n",
    "mae_test = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Test MAE:\", mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d45c708-aa81-40c7-bb39-00b222a4c668",
   "metadata": {},
   "source": [
    "#### combinations of kernels in the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b13d07-3b52-48e5-acea-b5ee8acc9cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process.kernels import Sum, Product, RBF, Matern, ConstantKernel, WhiteKernel\n",
    "\n",
    "# # Define the provided kernels\n",
    "# kernel_linear = ConstantKernel() * DotProduct()  # Linear kernel\n",
    "# kernel_rbf = RBF()  # RBF kernel\n",
    "# kernel_matern32 = Matern(length_scale=1.0, nu=1.5)  # Matern 3/2 kernel\n",
    "# kernel_bias = ConstantKernel()  # Bias kernel\n",
    "\n",
    "# # Combine the kernels\n",
    "# kernel_combined = (Product(kernel_linear, kernel_rbf) +\n",
    "#                    kernel_matern32 +\n",
    "#                    kernel_bias +\n",
    "#                    WhiteKernel(noise_level=0.1))  # Add a white noise kernel for regularization\n",
    "\n",
    "# # Define other parameters for the Gaussian Process model (if applicable)\n",
    "# random_state = 42  # Set a random seed for reproducibility\n",
    "\n",
    "# # Define the Gaussian Process Regression (GPR) model\n",
    "# gp_model_cv = GaussianProcessRegressor(kernel=kernel_combined, random_state=random_state, n_restarts_optimizer=4,\n",
    "#                                         alpha=0.5, normalize_y=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82efde6-4c67-4f27-adc4-8bec5d3c3616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't understand why model only predicting zeros, tried all possible approaches\n",
    "cv_predictions = gp_model_cv.predict(X_test)\n",
    "cv_predictions.shape\n",
    "X_test[:, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f602c27-7a23-4c63-b2aa-d3c800be075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Define a custom function to generate the Gaussian regression line\n",
    "def gaussian_regression_line(model):\n",
    "    # Generate x_values with 32 dimensions\n",
    "    x_values = np.full((10, 32), 0)  # Fill array with 0.5\n",
    "    x_values[:, 15] = np.linspace(0, 1, 10)  # Set the 15th column values to evenly spaced values from 0 to 1\n",
    "\n",
    "    # Predict mean and standard deviation of the Gaussian process at each point in x_values\n",
    "    y_mean, y_std = model.predict(x_values, return_std=True)\n",
    "\n",
    "    # Plot the mean line\n",
    "    plt.plot(x_values[:, 15], y_mean, color='blue', linestyle='-', label='Gaussian Regression Line')\n",
    "\n",
    "    # Calculate the z-score for the 99.7% confidence interval\n",
    "    z_score = norm.ppf(0.99865)  # 99.7th percentile (three standard deviations)\n",
    "\n",
    "    # Plot the shaded region for the 99.7% confidence interval with three standard deviations\n",
    "    plt.fill_between(x_values[:, 15], y_mean - z_score * y_std, y_mean + z_score * y_std, color='lightblue', alpha=0.3)\n",
    "\n",
    "\n",
    "    # Plot the z-score value on the plot\n",
    "    plt.text(0.5, 0.5, f'99.7% CI (z-score: {z_score:.2f})', transform=plt.gca().transAxes, fontsize=10, verticalalignment='bottom')\n",
    "\n",
    "# Plot the actual training data against the predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_test[:, 15], cv_predictions, color='red', label='Training Data')\n",
    "gaussian_regression_line(gp_model_cv)\n",
    "plt.xlabel('Leafcn values between 0 and 1')\n",
    "plt.ylabel('leafn  set to 1, rest all parameters to 0.5')\n",
    "plt.title('Actual Training Data vs Gaussian Regression Line with 99.7% Confidence Interval')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4600e5-ee67-4d6b-af56-cd812a8e0d7c",
   "metadata": {},
   "source": [
    "#### Sensitivity test for all individual parameters for leafn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21badf3f-40fa-4ba5-a657-3e9ebbfafcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Define a custom function to generate the Gaussian regression line for each parameter\n",
    "def gaussian_regression_lines(model, X, parameter_names):\n",
    "    plt.figure(figsize=(15, 10))  # Adjust figure size as needed\n",
    "    \n",
    "    # Initialize variables to store min and max y values globally\n",
    "    min_y_value_global = float('inf')\n",
    "    max_y_value_global = float('-inf')\n",
    "    \n",
    "    for param_index in range(32):\n",
    "        # Generate x_values with 32 dimensions\n",
    "        x_values = np.full((10, 32), 0.5)  # Fill array with 0.5\n",
    "        x_values[:, param_index] = np.linspace(0, 1, 10)  # Set the current parameter values to evenly spaced values from 0 to 1\n",
    "\n",
    "        # Predict mean and standard deviation of the Gaussian process at each point in x_values\n",
    "        y_mean, y_std = model.predict(x_values, return_std=True)\n",
    "        \n",
    "        # Calculate the z-score for the 99.7% confidence interval\n",
    "        z_score = norm.ppf(0.99865)  # 99.7th percentile (three standard deviations)\n",
    "\n",
    "        # Calculate y values for the 99.7% confidence interval\n",
    "        y_lower = y_mean - z_score * y_std\n",
    "        y_upper = y_mean + z_score * y_std\n",
    "        \n",
    "        # Update global min and max y values\n",
    "        min_y_value_global = min(min_y_value_global, np.min(y_lower))\n",
    "        max_y_value_global = max(max_y_value_global, np.max(y_upper))\n",
    "\n",
    "    for param_index in range(32):\n",
    "        # Generate x_values with 32 dimensions\n",
    "        x_values = np.full((10, 32), 0.5)  # Fill array with 0.5\n",
    "        x_values[:, param_index] = np.linspace(0, 1, 10)  # Set the current parameter values to evenly spaced values from 0 to 1\n",
    "\n",
    "        # Predict mean and standard deviation of the Gaussian process at each point in x_values\n",
    "        y_mean, y_std = model.predict(x_values, return_std=True)\n",
    "\n",
    "        # Plot the mean line for the current parameter\n",
    "        plt.subplot(4, 8, param_index + 1)  # Adjust subplot layout according to the number of parameters\n",
    "        plt.plot(x_values[:, param_index], y_mean, color='blue', linestyle='-', label='Gaussian Regression Line')\n",
    "\n",
    "        # Plot the shaded region for the 99.7% confidence interval with three standard deviations\n",
    "        plt.fill_between(x_values[:, param_index], y_mean - z_score * y_std, y_mean + z_score * y_std, color='lightblue', alpha=0.3)\n",
    "        \n",
    "        # Set y-axis limit for each subplot\n",
    "        plt.ylim(min_y_value_global, max_y_value_global)\n",
    "\n",
    "        # Plot the z-score value on the plot\n",
    "        plt.text(0.5, 0.5, f'99.7% CI (z-score: {z_score:.2f})', transform=plt.gca().transAxes, fontsize=8, verticalalignment='bottom')\n",
    "        \n",
    "        # Set title for each subplot based on the parameter name from the dictionary\n",
    "        plt.title(parameter_names[param_index])\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Replace model, X, and parameter_names with your actual data\n",
    "# gaussian_regression_lines(model, X, parameter_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb61b75-98d7-4a4f-8d3f-99734ee397ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Gaussian regression lines for each parameter\n",
    "gaussian_regression_lines(gp_model_cv, X_test, list(names_of_parameters_dict.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892dc609-37ca-4c17-9185-31fce7ea6e99",
   "metadata": {},
   "source": [
    "### FAST: Fourier Amplitude Sensitivity Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81357dc-0f68-44e9-97ba-79d3fab36f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from scipy.fft import fft\n",
    "\n",
    "# Define a custom function to generate the Gaussian regression line for each parameter\n",
    "def gaussian_regression_lines(model):\n",
    "    fourier_amplitudes = []  # List to store Fourier amplitudes for each parameter\n",
    "    \n",
    "    for param_index in range(32):\n",
    "        # Generate x_values with 32 dimensions\n",
    "        x_values = np.full((10, 32), 0.5)  # Fill array with 0.5\n",
    "        x_values[:, param_index] = np.linspace(0, 1, 10)  # Set the current parameter values to evenly spaced values from 0 to 1\n",
    "\n",
    "        # Predict mean and standard deviation of the Gaussian process at each point in x_values\n",
    "        y_mean, _ = model.predict(x_values, return_std=True)\n",
    "\n",
    "        # Compute Fourier transform of the model output\n",
    "        y_fft = fft(y_mean)\n",
    "\n",
    "        # Compute amplitude of each frequency component\n",
    "        amplitude = np.abs(y_fft)\n",
    "\n",
    "        # Store the amplitude corresponding to the first non-zero frequency (excluding DC component)\n",
    "        fourier_amplitudes.append(amplitude[1])\n",
    "\n",
    "    return fourier_amplitudes\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'model' and 'X' are already defined\n",
    "# Assuming 'names_of_parameters_dict' contains the parameter names dictionary\n",
    "\n",
    "fourier_amplitudes = gaussian_regression_lines(gp_model_cv)\n",
    "\n",
    "# Sort parameters based on Fourier amplitudes in descending order\n",
    "sorted_indices = np.argsort(fourier_amplitudes)[::-1]\n",
    "sorted_fourier_amplitudes = np.array(fourier_amplitudes)[sorted_indices]\n",
    "\n",
    "# Extract parameter names corresponding to sorted indices from lookup table\n",
    "sorted_parameter_names = [names_of_parameters_dict[index] for index in sorted_indices]\n",
    "\n",
    "# Plot horizontal bar chart\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(sorted_fourier_amplitudes)), sorted_fourier_amplitudes, color='#50c878')\n",
    "plt.ylabel('')\n",
    "plt.xlabel('Fourier Amplitude')\n",
    "plt.title('Sensitivity Test - Fourier Amplitudes(FAST)')\n",
    "plt.yticks(range(len(sorted_fourier_amplitudes)), sorted_parameter_names)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf3b9c1-412c-43d1-82f0-28297bf89327",
   "metadata": {},
   "source": [
    "### Sobol Sensivity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b64ce90-a498-4aca-b34c-be60dbf9e508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from SALib.sample import saltelli\n",
    "# from SALib.analyze import sobol\n",
    "\n",
    "# # Define the problem dictionary\n",
    "# problem = {\n",
    "#     'num_vars': 32,  # Number of parameters\n",
    "#     'names': [f'param_{i}' for i in range(32)],  # Parameter names\n",
    "#     'bounds': [[0, 1]] * 32  # Parameter bounds (assuming they vary between 0 and 1)\n",
    "# }\n",
    "\n",
    "# # Define a custom function to perform Sobol sensitivity analysis\n",
    "# def sobol_sensitivity_analysis(model, problem):\n",
    "#     # Generate samples using Saltelli's extension to Sobol sequence\n",
    "#     param_values = saltelli.sample(problem, 1000, calc_second_order=False)\n",
    "\n",
    "#     # Run the model and obtain outputs\n",
    "#     model_outputs, _ = model.predict(param_values, return_std=True)\n",
    "\n",
    "#     # Perform Sobol sensitivity analysis\n",
    "#     sobol_indices = sobol.analyze(problem, model_outputs, calc_second_order=False)\n",
    "\n",
    "#     # Get the first-order and total Sobol indices\n",
    "#     first_order_indices = sobol_indices['S1']\n",
    "#     total_indices = sobol_indices['ST']\n",
    "\n",
    "#     return first_order_indices, total_indices\n",
    "\n",
    "# # Example usage:\n",
    "# # Assuming 'gp_model_cv' and 'problem' are already defined\n",
    "# first_order_indices, total_indices = sobol_sensitivity_analysis(gp_model_cv, problem)\n",
    "\n",
    "# # Plotting\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# plt.barh(range(len(first_order_indices)), first_order_indices, color='blue', label='First Order Sobol Indices')\n",
    "# plt.barh(range(len(total_indices)), total_indices, color='red', label='Total Sobol Indices')\n",
    "# plt.xlabel('Sobol Index Value')\n",
    "# plt.ylabel('Parameter Index')\n",
    "# plt.title('Sobol Sensitivity Analysis')\n",
    "# plt.yticks(range(len(first_order_indices)), range(32))  # Assuming parameter indices are numbered from 0 to 31\n",
    "# plt.legend()\n",
    "# plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ad2d63-4d41-463a-8db5-d7eee2901ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Define a custom function to generate the Gaussian regression line for each parameter\n",
    "def gaussian_regression_lines(model, X, parameter_names):\n",
    "    plt.figure(figsize=(15, 10))  # Adjust figure size as needed\n",
    "    \n",
    "    # Initialize variables to store min and max y values globally\n",
    "    min_y_value_global = float('inf')\n",
    "    max_y_value_global = float('-inf')\n",
    "    \n",
    "    for param_index in range(32):\n",
    "        # Generate x_values with 32 dimensions\n",
    "        x_values = np.full((10, 32), 0.5)  # Fill array with 0.5\n",
    "        x_values[:, param_index] = np.linspace(0, 1, 10)  # Set the current parameter values to evenly spaced values from 0 to 1\n",
    "\n",
    "        # Predict mean and standard deviation of the Gaussian process at each point in x_values\n",
    "        y_mean, y_std = model.predict(x_values, return_std=True)\n",
    "        \n",
    "        # Calculate the z-score for the 99.7% confidence interval\n",
    "        z_score = norm.ppf(0.99865)  # 99.7th percentile (three standard deviations)\n",
    "\n",
    "        # Calculate y values for the 99.7% confidence interval\n",
    "        y_lower = y_mean - z_score * y_std\n",
    "        y_upper = y_mean + z_score * y_std\n",
    "        \n",
    "        # Update global min and max y values\n",
    "        min_y_value_global = min(min_y_value_global, np.min(y_lower))\n",
    "        max_y_value_global = max(max_y_value_global, np.max(y_upper))\n",
    "\n",
    "    for param_index in range(32):\n",
    "        # Generate x_values with 32 dimensions\n",
    "        x_values = np.full((10, 32), 0.5)  # Fill array with 0.5\n",
    "        x_values[:, param_index] = np.linspace(0, 1, 10)  # Set the current parameter values to evenly spaced values from 0 to 1\n",
    "\n",
    "        # Predict mean and standard deviation of the Gaussian process at each point in x_values\n",
    "        y_mean, y_std = model.predict(x_values, return_std=True)\n",
    "\n",
    "        # Plot the mean line for the current parameter\n",
    "        plt.subplot(4, 8, param_index + 1)  # Adjust subplot layout according to the number of parameters\n",
    "        plt.plot(x_values[:, param_index], y_mean, color='blue', linestyle='-', label='Gaussian Regression Line')\n",
    "\n",
    "        # Plot the shaded region for the 99.7% confidence interval with three standard deviations\n",
    "        plt.fill_between(x_values[:, param_index], y_mean - z_score * y_std, y_mean + z_score * y_std, color='lightblue', alpha=0.3)\n",
    "        \n",
    "        # Set y-axis limit for each subplot\n",
    "        plt.ylim(min_y_value_global, max_y_value_global)\n",
    "\n",
    "        # Plot the z-score value on the plot\n",
    "        plt.text(0.5, 0.5, f'99.7% CI (z-score: {z_score:.2f})', transform=plt.gca().transAxes, fontsize=8, verticalalignment='bottom')\n",
    "        \n",
    "        # Set title for each subplot based on the parameter name from the dictionary\n",
    "        plt.title(parameter_names[param_index])\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Replace model, X, and parameter_names with your actual data\n",
    "# gaussian_regression_lines(model, X, parameter_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbac1f11-cbf3-4a44-8efa-2ffeceb60f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_names = xr.open_dataset('/glade/campaign/cgd/tss/projects/PPE/PPEn11_LHC/paramfiles/LHC0000.nc')\n",
    "full_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bec741-0b24-4432-b091-5987b4e62302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the frozen dictionary and print variable names\n",
    "#full_names[''].attrs['long_name']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-meds-py]",
   "language": "python",
   "name": "conda-env-.conda-meds-py-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
